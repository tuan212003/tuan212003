{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xác định thực thể trong câu bằng thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xác định bằng thư viện Tiếng Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuyen ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text_en = \"My name is Tuyen\"\n",
    "\n",
    "doc_en = nlp_en(text_en)\n",
    "\n",
    "for ent in doc_en.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xác định bằng thư viện Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "# import requests\n",
    "\n",
    "# url = \"https://github.com/vncorenlp/VnCoreNLP/archive/v1.1.1.zip\"\n",
    "# zip_path = \"VnCoreNLP-1.1.1.zip\"\n",
    "\n",
    "# if not os.path.exists(zip_path):\n",
    "#     r = requests.get(url)\n",
    "#     with open(zip_path, 'wb') as f:\n",
    "#         f.write(r.content)\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall()\n",
    "\n",
    "\n",
    "jar_path = \"/root/code/Thuvien/VnCoreNLP-1.1.1/VnCoreNLP-1.1.1.jar\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Từ: trường, POS: N, NER: B-ORG\n",
      "Từ: Nguyễn_Trường_Tộ, POS: Np, NER: I-ORG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "vncorenlp = VnCoreNLP(jar_path, annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g')\n",
    "\n",
    "def print_named_entities(text):\n",
    "    annotations = vncorenlp.annotate(text)\n",
    "    for sentence in annotations['sentences']:\n",
    "        for token in sentence:\n",
    "            if token['nerLabel'] != 'O':\n",
    "                print(f\"Từ: {token['form']}, POS: {token['posTag']}, NER: {token['nerLabel']}\")\n",
    "\n",
    "text_vi = \"Hôm nay tôi đi học tại trường Nguyễn Trường Tộ\"\n",
    "\n",
    "print_named_entities(text_vi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Từ: Nguyễn_Trường_Tộ, POS: Np, NER: B-PER\n",
      "Từ: Hà_Nội, POS: Np, NER: B-LOC\n"
     ]
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "vncorenlp = VnCoreNLP(jar_path, annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g')\n",
    "\n",
    "def print_named_entities(text):\n",
    "    annotations = vncorenlp.annotate(text)\n",
    "    for sentence in annotations['sentences']:\n",
    "        for token in sentence:\n",
    "            if token['nerLabel'] != 'O':\n",
    "                print(f\"Từ: {token['form']}, POS: {token['posTag']}, NER: {token['nerLabel']}\")\n",
    "\n",
    "text_vi = \"Nguyễn Trường Tộ đang đi học tại Hà Nội\"\n",
    "\n",
    "print_named_entities(text_vi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xác định bằng thuật toán tự code, sử dụng cho Tiếng Việt MiAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model, load_model\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_contrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CRF\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_to_word_sequence\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_contrib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import Input\n",
    "from keras_contrib.layers import CRF\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from cls_sentence import sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_962/4021734431.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method = 'ffill')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"./ner_dataset.csv\", encoding = \"ISO-8859-1\")\n",
    "df = df.fillna(method = 'ffill')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,407,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">918</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │     \u001b[38;5;34m1,407,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m36,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │         \u001b[38;5;34m5,050\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m18\u001b[0m)         │           \u001b[38;5;34m918\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,449,528</span> (5.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,449,528\u001b[0m (5.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,449,528</span> (5.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,449,528\u001b[0m (5.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 23:04:49.350953: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 198115200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 121ms/step - accuracy: 0.9388 - loss: 0.3295 - val_accuracy: 0.9638 - val_loss: 0.1289\n",
      "Epoch 2/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 120ms/step - accuracy: 0.9671 - loss: 0.1139 - val_accuracy: 0.9750 - val_loss: 0.0880\n",
      "Epoch 3/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 118ms/step - accuracy: 0.9783 - loss: 0.0788 - val_accuracy: 0.9835 - val_loss: 0.0631\n",
      "Epoch 4/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 125ms/step - accuracy: 0.9851 - loss: 0.0556 - val_accuracy: 0.9861 - val_loss: 0.0506\n",
      "Epoch 5/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 122ms/step - accuracy: 0.9881 - loss: 0.0424 - val_accuracy: 0.9875 - val_loss: 0.0435\n",
      "Epoch 6/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 139ms/step - accuracy: 0.9895 - loss: 0.0366 - val_accuracy: 0.9885 - val_loss: 0.0400\n",
      "Epoch 7/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 118ms/step - accuracy: 0.9906 - loss: 0.0324 - val_accuracy: 0.9885 - val_loss: 0.0413\n",
      "Epoch 8/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 119ms/step - accuracy: 0.9914 - loss: 0.0297 - val_accuracy: 0.9889 - val_loss: 0.0386\n",
      "Epoch 9/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 120ms/step - accuracy: 0.9919 - loss: 0.0280 - val_accuracy: 0.9897 - val_loss: 0.0360\n",
      "Epoch 10/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 122ms/step - accuracy: 0.9922 - loss: 0.0266 - val_accuracy: 0.9896 - val_loss: 0.0367\n",
      "Epoch 11/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 125ms/step - accuracy: 0.9928 - loss: 0.0248 - val_accuracy: 0.9899 - val_loss: 0.0358\n",
      "Epoch 12/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 125ms/step - accuracy: 0.9929 - loss: 0.0243 - val_accuracy: 0.9901 - val_loss: 0.0353\n",
      "Epoch 13/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 125ms/step - accuracy: 0.9931 - loss: 0.0233 - val_accuracy: 0.9902 - val_loss: 0.0356\n",
      "Epoch 14/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 121ms/step - accuracy: 0.9933 - loss: 0.0224 - val_accuracy: 0.9902 - val_loss: 0.0344\n",
      "Epoch 15/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 126ms/step - accuracy: 0.9936 - loss: 0.0215 - val_accuracy: 0.9903 - val_loss: 0.0341\n",
      "Epoch 16/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 123ms/step - accuracy: 0.9938 - loss: 0.0209 - val_accuracy: 0.9905 - val_loss: 0.0338\n",
      "Epoch 17/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 122ms/step - accuracy: 0.9939 - loss: 0.0202 - val_accuracy: 0.9900 - val_loss: 0.0366\n",
      "Epoch 18/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 121ms/step - accuracy: 0.9941 - loss: 0.0195 - val_accuracy: 0.9904 - val_loss: 0.0338\n",
      "Epoch 19/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 122ms/step - accuracy: 0.9943 - loss: 0.0189 - val_accuracy: 0.9903 - val_loss: 0.0339\n",
      "Epoch 20/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34167s\u001b[0m 60s/step - accuracy: 0.9943 - loss: 0.0188 - val_accuracy: 0.9902 - val_loss: 0.0365\n",
      "Epoch 21/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 163ms/step - accuracy: 0.9944 - loss: 0.0183 - val_accuracy: 0.9905 - val_loss: 0.0345\n",
      "Epoch 22/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 229ms/step - accuracy: 0.9945 - loss: 0.0181 - val_accuracy: 0.9901 - val_loss: 0.0353\n",
      "Epoch 23/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 210ms/step - accuracy: 0.9946 - loss: 0.0176 - val_accuracy: 0.9903 - val_loss: 0.0363\n",
      "Epoch 24/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 164ms/step - accuracy: 0.9948 - loss: 0.0170 - val_accuracy: 0.9906 - val_loss: 0.0346\n",
      "Epoch 25/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 170ms/step - accuracy: 0.9950 - loss: 0.0164 - val_accuracy: 0.9902 - val_loss: 0.0352\n",
      "Epoch 26/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 202ms/step - accuracy: 0.9950 - loss: 0.0160 - val_accuracy: 0.9904 - val_loss: 0.0364\n",
      "Epoch 27/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 190ms/step - accuracy: 0.9951 - loss: 0.0158 - val_accuracy: 0.9899 - val_loss: 0.0364\n",
      "Epoch 28/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 155ms/step - accuracy: 0.9952 - loss: 0.0157 - val_accuracy: 0.9899 - val_loss: 0.0372\n",
      "Epoch 29/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 208ms/step - accuracy: 0.9953 - loss: 0.0153 - val_accuracy: 0.9902 - val_loss: 0.0364\n",
      "Epoch 30/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 214ms/step - accuracy: 0.9954 - loss: 0.0147 - val_accuracy: 0.9900 - val_loss: 0.0370\n",
      "Epoch 31/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 186ms/step - accuracy: 0.9954 - loss: 0.0149 - val_accuracy: 0.9900 - val_loss: 0.0373\n",
      "Epoch 32/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 152ms/step - accuracy: 0.9956 - loss: 0.0143 - val_accuracy: 0.9901 - val_loss: 0.0374\n",
      "Epoch 33/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 164ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9896 - val_loss: 0.0398\n",
      "Epoch 34/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 136ms/step - accuracy: 0.9958 - loss: 0.0135 - val_accuracy: 0.9897 - val_loss: 0.0383\n",
      "Epoch 35/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 132ms/step - accuracy: 0.9958 - loss: 0.0133 - val_accuracy: 0.9901 - val_loss: 0.0410\n",
      "Epoch 36/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 134ms/step - accuracy: 0.9961 - loss: 0.0128 - val_accuracy: 0.9887 - val_loss: 0.0418\n",
      "Epoch 37/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 167ms/step - accuracy: 0.9961 - loss: 0.0127 - val_accuracy: 0.9902 - val_loss: 0.0413\n",
      "Epoch 38/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 176ms/step - accuracy: 0.9961 - loss: 0.0125 - val_accuracy: 0.9889 - val_loss: 0.0421\n",
      "Epoch 39/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 187ms/step - accuracy: 0.9962 - loss: 0.0122 - val_accuracy: 0.9899 - val_loss: 0.0416\n",
      "Epoch 40/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 152ms/step - accuracy: 0.9963 - loss: 0.0117 - val_accuracy: 0.9895 - val_loss: 0.0406\n",
      "Epoch 41/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 178ms/step - accuracy: 0.9964 - loss: 0.0113 - val_accuracy: 0.9900 - val_loss: 0.0412\n",
      "Epoch 42/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 152ms/step - accuracy: 0.9965 - loss: 0.0113 - val_accuracy: 0.9900 - val_loss: 0.0422\n",
      "Epoch 43/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 156ms/step - accuracy: 0.9965 - loss: 0.0112 - val_accuracy: 0.9898 - val_loss: 0.0423\n",
      "Epoch 44/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 161ms/step - accuracy: 0.9967 - loss: 0.0107 - val_accuracy: 0.9896 - val_loss: 0.0422\n",
      "Epoch 45/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 130ms/step - accuracy: 0.9967 - loss: 0.0105 - val_accuracy: 0.9895 - val_loss: 0.0428\n",
      "Epoch 46/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 142ms/step - accuracy: 0.9967 - loss: 0.0104 - val_accuracy: 0.9898 - val_loss: 0.0448\n",
      "Epoch 47/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 124ms/step - accuracy: 0.9969 - loss: 0.0101 - val_accuracy: 0.9896 - val_loss: 0.0444\n",
      "Epoch 48/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 131ms/step - accuracy: 0.9969 - loss: 0.0099 - val_accuracy: 0.9893 - val_loss: 0.0453\n",
      "Epoch 49/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 132ms/step - accuracy: 0.9970 - loss: 0.0097 - val_accuracy: 0.9893 - val_loss: 0.0450\n",
      "Epoch 50/50\n",
      "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 132ms/step - accuracy: 0.9970 - loss: 0.0095 - val_accuracy: 0.9895 - val_loss: 0.0446\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score is : 85.1%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Example #1681\n",
      "Word           ||True ||Pred\n",
      "****************************************\n",
      "The            : O     O\n",
      "Israeli        : B-gpe B-gpe\n",
      "military       : O     O\n",
      "said           : O     O\n",
      "soldiers       : O     O\n",
      "have           : O     O\n",
      "uncovered      : O     O\n",
      "at             : O     O\n",
      "least          : O     O\n",
      "three          : O     O\n",
      "explosives     : O     O\n",
      "laboratories   : O     O\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from cls_sentence import sentence\n",
    "\n",
    "# Config\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "max_len = 75\n",
    "embedding = 40\n",
    "\n",
    "def load_data(filename='ner_dataset.csv'):\n",
    "    df = pd.read_csv(filename, encoding=\"ISO-8859-1\")\n",
    "    df = df.fillna(method='ffill')\n",
    "    return df\n",
    "\n",
    "def process_data(df, sentences):\n",
    "    words = list(df['Word'].unique())\n",
    "    tags = list(df['Tag'].unique())\n",
    "\n",
    "    word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "    word2idx[\"UNK\"] = 1\n",
    "    word2idx[\"PAD\"] = 0\n",
    "\n",
    "    tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "    tag2idx[\"PAD\"] = 0\n",
    "\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "    X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "    y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "\n",
    "    num_tag = df['Tag'].nunique()\n",
    "    y = [to_categorical(i, num_classes=num_tag + 1) for i in y]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, word2idx, tag2idx, idx2word, idx2tag, num_tag, words, tags\n",
    "\n",
    "def build_model(num_tags, hidden_size=50):\n",
    "    input = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=len(words) + 2, output_dim=embedding, input_length=max_len, mask_zero=False)(input)\n",
    "    model = Bidirectional(LSTM(units=hidden_size, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    model = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(model)\n",
    "    out = TimeDistributed(Dense(num_tags + 1, activation=\"softmax\"))(model)  # Thay thế CRF bằng lớp Dense\n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(\"data.pkl\"):\n",
    "    print(\"Data not found, make it!\")\n",
    "    df = load_data()\n",
    "    getter = sentence(df)\n",
    "    sentences = getter.sentences\n",
    "    X_train, X_test, y_train, y_test, word2idx, tag2idx, idx2word, idx2tag, num_tag, words, tags = process_data(df, sentences)\n",
    "    with open('data.pkl', 'wb') as file:\n",
    "        data = [X_train, X_test, y_train, y_test, word2idx, tag2idx, idx2word, idx2tag, num_tag, words, tags]\n",
    "        pickle.dump(data, file)\n",
    "else:\n",
    "    with open(\"data.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        X_train, X_test, y_train, y_test, word2idx, tag2idx, idx2word, idx2tag, num_tag, words, tags = data\n",
    "\n",
    "if not os.path.exists(\"model.keras\"):\n",
    "    model = build_model(num_tag)\n",
    "    checkpoint = ModelCheckpoint(filepath='model.keras',\n",
    "                                 verbose=0,\n",
    "                                 mode='auto',\n",
    "                                 save_best_only=True,\n",
    "                                 monitor='val_loss')\n",
    "    history = model.fit(X_train, np.array(y_train), batch_size=batch_size, epochs=epochs,\n",
    "                        validation_split=0.1, callbacks=[checkpoint])\n",
    "else:\n",
    "    model = build_model(num_tag)\n",
    "    model.load_weights(\"model.keras\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_test_true = np.argmax(y_test, -1)\n",
    "\n",
    "y_pred = [[idx2tag[i] for i in row] for row in y_pred]\n",
    "y_test_true = [[idx2tag[i] for i in row] for row in y_test_true]\n",
    "print(\"F1-score is : {:.1%}\".format(f1_score(y_test_true, y_pred)))\n",
    "\n",
    "idx = np.random.randint(0, X_test.shape[0])\n",
    "\n",
    "p = model.predict(np.array([X_test[idx]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_test[idx], -1)\n",
    "\n",
    "print(\"Example #{}\".format(idx))\n",
    "\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(40 * \"*\")\n",
    "for w, t, pred in zip(X_test[idx], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-2], idx2tag[t], idx2tag[pred]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Example #6078\n",
      "Word           ||True ||Pred\n",
      "****************************************\n",
      "The            : O     O\n",
      "Athens         : B-org B-org\n",
      "Geodynamic     : I-org I-org\n",
      "Institute      : I-org I-org\n",
      "says           : O     O\n",
      "Friday         : B-tim B-tim\n",
      "'s             : O     O\n",
      "magnitude      : O     O\n",
      "5.1            : O     O\n",
      "quake          : O     O\n",
      "was            : O     O\n",
      "centered       : O     O\n",
      "near           : O     O\n",
      "the            : O     O\n",
      "eastern        : O     O\n",
      "islands        : O     O\n",
      "of             : O     O\n",
      "Kos            : B-geo B-geo\n",
      "and            : O     O\n",
      "Astypaleia     : B-geo B-geo\n",
      ",              : O     O\n",
      "about          : O     O\n",
      "300            : O     O\n",
      "kilometers     : O     O\n",
      "from           : O     O\n",
      "the            : O     O\n",
      "Greek          : B-gpe B-gpe\n",
      "capital        : O     O\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "# Test với một câu ngẫu nhiên trong tập test\n",
    "idx = np.random.randint(0,X_test.shape[0])\n",
    "\n",
    "p = model.predict(np.array([X_test[idx]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_test[idx], -1)\n",
    "\n",
    "print(\"Example #{}\".format(idx))\n",
    "\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(40 * \"*\")\n",
    "for w, t, pred in zip(X_test[idx], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-2], idx2tag[t], idx2tag[pred]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
